\documentclass[a4paper,10pt]{exam}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,citecolor=green]{hyperref}
\lhead{}
\chead{Quiz 3}
\rhead{}
\title{Homework of Machine Learning Techniques: Quiz 3}
\date{}
\author{}
\headrule
% \printanswers

\begin{document}
\maketitle
\section*{Decision Tree}
\begin{questions}
	\question Impurity functions play an important role in decision tree branching. For binary classification problems, let $\mu_+$ be the fraction of positive examples in a data subset, and $\mu_- = 1 - \mu_+$ be the fraction of negative examples in the data subset.
	The Gini index is $1 - \mu_+^2 - \mu_-^2$. What is the maximum value of the Gini index among all $\mu_+ \in [0, 1]$?
	\begin{checkboxes}
		\CorrectChoice 0.5
		\choice 0.75
		\choice 0.25
		\choice 0
		\choice 1\\
	\end{checkboxes}

	\question Following Question \ref{question@1}, there are four possible impurity functions below. We can normalize each impurity function by dividing it with its maximum value among all $\mu_+ \in [0, 1]$ For instance, the classification error is simply $\min(\mu_+, \mu_-)$ and its maximum value is 0.5. So the normalized classification error is $2 \min(\mu_+, \mu_-)$. After normalization, which of the following impurity function is equivalent to the normalized Gini index?

	\begin{checkboxes}
		\CorrectChoice the squared regression error (used for branching in classification data sets), which is by definition $\mu_+ (1 - (\mu_+ - \mu_-))^2 + \mu_- (-1 - (\mu_+ - \mu_-))^2$.
		\choice the entropy, which is $-\mu_+ \ln \mu_+ - \mu_- \ln \mu_-$, with $0 \log 0 \equiv 0$.
		\choice the closeness, which is $1 - |\mu_+ - \mu_-|$.
		\choice the classification error $\min(\mu_+, \mu_-)$.
		\choice none of the other choices\\
	\end{checkboxes}

	\section*{Random Forest}
	\question
	If bootstrapping is used to sample $N^\prime = pN$ examples out of $N$ examples and $N$ is very large. Approximately how many of the $N$ examples will not be sampled at all?
	\begin{checkboxes}
		\choice $(1 - e^{-1/p}) \cdot N$
		\choice $(1 - e^{-p}) \cdot N$
		\choice $e^{-1} \cdot N$
		\choice $e^{-1/p} \cdot N$
		\CorrectChoice $e^{-p} \cdot N$\\
	\end{checkboxes}

	\question Consider a Random Forest $G$ that consists of three binary classification trees $\{g_k\}_{k=1}^3$, where each tree is of test 0/1 error $E_{\text{out}}(g_1) = 0.1$, $E_{\text{out}}(g_2) = 0.2$, $E_{\text{out}}(g_3) = 0.3$. Which of the following is the exact possible range of $E_{\text{out}}(G)$?
	\begin{checkboxes}
		\choice $0 \le E_{\text{out}}(G) \le 0.1$
		\choice $0.1 \le E_{\text{out}}(G) \le 0.6$
		\choice $0.2 \le E_{\text{out}}(G) \le 0.3$
		\choice $0.1 \le E_{\text{out}}(G) \le 0.3$
		\CorrectChoice $0.1 \le E_{\text{out}}(G) \le 0.3$\\
	\end{checkboxes}

	\question Consider a Random Forest $G$ that consists of $K$ binary classification trees $\{g_k\}_{k=1}^K$, where $K$ is an odd integer. Each $g_k$ is of test 0/1 error $E_{\text{out}}(g_k) = e_k$. Which of the following is an upper bound of $E_{\text{out}}(G)$?
	\begin{checkboxes}
		\CorrectChoice $\frac{2}{K+1} \sum_{k=1}^K e_k$
		\choice $\frac{1}{K} \sum_{k=1}^K e_k$
		\choice $\frac{1}{K+1} \sum_{k=1}^K e_k$
		\choice $\min_{1 \le k \le K} e_k$
		\choice $\max_{1 \le k \le K} e_k$\\
	\end{checkboxes}

	\section*{Gradient Boosting}
	\question
	Let $\epsilon_t$ be the weighted 0/1 error of each $g_t$ as described in the AdaBoost algorithm (Lecture 208), and $U_t = \sum_{n=1}^N u_n^{(t)}$ be the total example weight during AdaBoost. Which of the following equation expresses $U_{T+1}$ by $\epsilon_t$?
	\begin{checkboxes}
		\choice none of the other choices
		\choice $\prod_{t=1}^T \epsilon_t$
		\choice $\sum_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$
		\choice $\sum_{t=1}^T \epsilon_t$
		\CorrectChoice $\prod_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$\\
	\end{checkboxes}

	\question For the gradient boosted decision tree, if a tree with only one constant node is returned as $g_1$, and if $g_1(\mathbf{x}) = 2$, then after the first iteration, all $s_n$ is updated from $0$ to a new constant $\alpha_1 g_1(\mathbf{x}_n)$. What is $s_n$?

	\begin{checkboxes}
		\choice 2
		\choice none of the other choices
		\choice $\max_{1 \le n \le N} y_n$
		\choice $\min_{1 \le n \le N} y_n$
		\CorrectChoice $\frac{1}{N} \sum_{n=1}^N y_n$\\
	\end{checkboxes}

	\question For the gradient boosted decision tree, after updating all $s_n$ in iteration $t$ using the steepest $\eta$ as $\alpha_t$, what is the value of $\sum_{n=1}^N s_n g_t(\mathbf{x}_n)$?

	\begin{checkboxes}
		\choice none of the other choices
		\CorrectChoice $\sum_{n=1}^N y_n g_t(\mathbf{x}_n)$
		\choice $\sum_{n=1}^N y_n^2$
		\choice $\sum_{n=1}^N y_n s_n$
		\choice 0	\\
	\end{checkboxes}

	\question \textbf{Neural Network}\\
	Consider Neural Network with $\mbox{sign}(s)$ instead of $\tanh(s)$ as the transformation functions. That is, consider Multi-Layer Perceptrons. In addition, we will take $+1$ to mean logic TRUE, and ${{ - }}1$ to mean logic FALSE. Assume that all $x_i$ below are either $+1$ or ${{ - }}1$. Which of the following perceptron
	\[g_A(\mathbf{x}) = \mbox{sign}\left(\sum_{i=0}^d w_i x_i\right).\]
	implements
	\[\text{OR}\left(x_1,x_2,\ldots,x_d\right).\]
	\begin{checkboxes}
		\CorrectChoice $(w_0, w_1, w_2, \cdots, w_d) = (d-1, +1, +1, \cdots, +1)$
		\choice $(w_0, w_1, w_2, \cdots, w_d) = (-d+1, -1, -1, \cdots, -1)$
		\choice none of the other choices
		\choice $(w_0, w_1, w_2, \cdots, w_d) = (d-1, -1, -1, \cdots, -1)$
		\choice $(w_0, w_1, w_2, \cdots, w_d) = (-d+1, +1, +1, \cdots, +1)$\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@9}, among the following choices of $D$, which $D$ is the smallest for some $5$-$D$-$1$ Neural Network to implement $\text{XOR}\bigl(x_1,x_2,x_3,x_4, x_5\bigr)$?
	\begin{checkboxes}
		\choice 1
		\choice 9
		\choice 7
		\CorrectChoice 5
		\choice 3\\
	\end{checkboxes}

	\question For a Neural Network with at least one hidden layer and $\tanh(s)$ as the transformation functions on all neurons (including the output neuron), what is true about the gradient components (with respect to the weights) when all the initial weights $w_{ij}^{(\ell)}$ are set to $0$?
	\begin{checkboxes}
		\choice all the gradient components are zero
		\choice only the gradient components with respect to $w_{0j}^{(\ell)}$ for $j > 0$ may non-zero, all other gradient components must be zero
		\choice none of the other choices
		\choice only the gradient components with respect to $w_{j1}^{(L)}$ for $j>0$ may be non-zero, all other gradient components must be zero
		\CorrectChoice only the gradient components with respect to $w_{01}^{(L)}$ may be non-zero, all other gradient components must be zero \\
	\end{checkboxes}

	\question For a Neural Network with one hidden layer and $\tanh(s)$ as the transformation functions on all neurons (including the output neuron), what is always true about the backprop algorithm when all the initial weights $w_{ij}^{(\ell)}$ are set to $1$?
	\begin{checkboxes}
		\choice none of the other choices
		\CorrectChoice $w_{ij}^{(1)} = w_{i(j+1)}^{(1)}$ for all $i$ and $1 \le j{\rm{  < }}{d^{(1)}} - 1$
		\choice all $w_{j1}^{(2)}$ for $j>0$ are different
		\choice $w_{ij}^{(1)} = w_{(i+1)j}^{(1)}$ for $1 \le i{\rm{  < }}{d^{(0)}} - 1$ and all $j$
		\choice the gradient components with respect to all $w_{ij}^{(\ell)}$ are zero\\
	\end{checkboxes}

	\section*{Experiments with Decision Tree}
	\question
	Implement the simple C\&RT algorithm without pruning using the Gini index as the impurity measure as introduced in the class. For the decision stump used in branching, if you are branching with feature $i$ and direction $s$, please sort all the $x_{n, i}$ values to form (at most) $N+1$ segments of equivalent $\theta$, and then pick $\theta$ within the median of the segment. Run the algorithm on the following set for training:

	\href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw3_data/hw3_train.dat}{hw3\_train.dat}

	and the following set for testing:

	\href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw3_data/hw3_test.dat}{hw3\_test.dat}

	How many internal nodes (branching functions) are there in the resulting tree $G$?
	\begin{checkboxes}
		\choice 12
		\choice 8
		\choice 14
		\CorrectChoice 10
		\choice 6\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@13}, which of the following is closest to the $E_{\text{in}}$ (evaluated with 0/1 error) of the tree?
	\begin{checkboxes}
		\CorrectChoice 0.0
		\choice 0.1
		\choice 0.2
		\choice 0.3
		\choice 0.4\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@13}, which of the following is closest to the $E_{\text{out}}$ (evaluated with 0/1 error) of the tree?
	\begin{checkboxes}
		\choice 0.05
		\choice 0.25
		\choice 0.35
		\choice 0.00
		\CorrectChoice 0.15\\
	\end{checkboxes}

	\question Now implement the Bagging algorithm with $N^\prime = N$ and couple it with your decision tree above to make a preliminary random forest $G_{RS}$. Produce $T=300$ trees with bagging. Repeat the experiment for 100 times and compute average $E_{\text{in}}$ and $E_{\text{out}}$ using the 0/1 error.
	Which of the following is true about the average $E_{\text{in}}(g_t)$ for all the 30000 trees that you have generated?
	\begin{checkboxes}
		\CorrectChoice $0.03 \leq \mbox{average } E_{\text{in}}(g_t) <0.06$
		\choice $0.00 \leq \mbox{average } E_{\text{in}}(g_t) < 0.03$
		\choice $0.09 \leq \mbox{average } E_{\text{in}}(g_t) < 0.12$
		\choice $0.06 \leq \mbox{average } E_{\text{in}}(g_t) < 0.09$
		\choice $0.12 \leq \mbox{average } E_{\text{in}}(g_t) < 0.50$\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@16}, which of the following is true about the average $E_{\text{in}}(G_{RF})$?
	\begin{checkboxes}
		\choice $0.06 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.09$
		\choice $0.09 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.12$
		\choice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.50$
		\CorrectChoice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.50$
		\choice $0.03 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.06$\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@16}, which of the following is true about the average $E_{\text{out}}(G_{RF})$?
	\begin{checkboxes}
		\CorrectChoice $0.06 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.09$
		\choice $0.09 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.12$
		\choice $0.03 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.06$
		\choice $0.00 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.03$
		\choice $0.12 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.50$\\
	\end{checkboxes}

	\question Now, 'prune' your decision tree algorithm by restricting it to have one branch only. That is, the tree is simply a decision stump determined by Gini index. Make a random 'forest' $G_{RS}$ with those decision stumps with Bagging like Questions 16-18 with $T=300$. Repeat the experiment for 100 times and compute average $E_{\text{in}}$ and $E_{\text{out}}$ using the 0/1 error.
	Which of the following is true about the average $E_{\text{in}}(G_{RS})$?

	\begin{checkboxes}
		\CorrectChoice $0.09 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.12$
		\choice $0.03 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.06$
		\choice $0.00 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.03$
		\choice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.50$
		\choice $0.06 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.09$\\
	\end{checkboxes}

	\question Continuing from Question \ref{question@19}, which of the following is true about the average $E_{\text{out}}(G_{RS})$?
	\begin{checkboxes}
		\choice $0.06 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.09$
		\choice $0.09 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.12$
		\choice $0.03 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.06$
		\choice $0.00 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.03$
		\CorrectChoice $0.12 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.50$\\
	\end{checkboxes}
\end{questions}
\end{document}
